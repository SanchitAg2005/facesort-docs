# FaceSort System Architecture

This document explains the high-level architecture of FaceSort and how its components interact to process, match, and deliver photos in a scalable and privacy-focused way.

The system follows a **hybrid cloudâ€“local architecture**, where orchestration happens in the cloud while computationally heavy face-recognition tasks run on a controlled local worker.

---

## Architectural Goals

The architecture was designed around several core goals:

- Reduce cloud compute costs by offloading heavy AI processing
- Maintain user privacy by limiting biometric processing in the cloud
- Allow asynchronous job execution using queue-based workflows
- Support modular upgrades without rewriting the entire system
- Ensure safe cleanup and predictable system states

---

## High-Level Components

### 1. Frontend (Next.js)

The frontend provides the user interface for:

- Profile management
- Friend enrollment
- Image uploads
- Job monitoring

Responsibilities:

- Client-side image resizing for faster uploads
- Sending requests to backend APIs
- Displaying real-time job and friend statuses

The frontend does not perform any face-recognition logic.

---

### 2. Backend API (FastAPI)

The backend acts as the orchestration layer.

Primary responsibilities:

- Authentication-aware API routes
- Managing job creation and lifecycle states
- Writing and reading metadata from the database
- Generating signed URLs for secure storage access
- Coordinating between storage, worker, and messaging systems

The backend is intentionally lightweight and avoids heavy AI processing.

---

### 3. Database Layer (MongoDB)

MongoDB stores structured metadata for:

- Users
- Friends
- Jobs
- Telegram connections

The database tracks system state transitions such as:

- queued
- processing
- matched
- delivered
- completed
- error

This enables clear tracking of long-running asynchronous operations.

---

### 4. Object Storage (Supabase Storage)

Supabase is used strictly as an object storage layer.

Stored assets include:

- Uploaded batch images
- Friend thumbnails
- Face embedding files

Design considerations:

- Files are accessed using signed URLs
- Storage paths are referenced from MongoDB
- Temporary files are deleted after processing

Supabase does not run any recognition logic.

---

### 5. Local Worker (Face Recognition Engine)

The local worker is responsible for computational tasks.

Responsibilities include:

- Fetching queued jobs from backend
- Downloading images securely
- Running face detection and embedding extraction
- Matching faces against stored embeddings
- Building structured result logs
- Triggering delivery workflows

The worker operates independently from the frontend and backend servers, allowing flexible deployment.

---

### 6. Messaging Layer (Telegram Bot)

The messaging layer delivers matched photos to recipients.

Responsibilities:

- Sending images to predefined chat IDs
- Managing delivery confirmation
- Handling messaging retries if required

This layer runs after matching is complete.

---

## Data Flow Overview

A typical processing cycle:

1. User uploads images from the frontend
2. Backend stores metadata and creates a job
3. Images are uploaded to object storage
4. Worker polls backend and detects queued jobs
5. Worker downloads images and performs matching
6. Results are written back to the database
7. Messaging layer delivers photos
8. Temporary files are cleaned up

Each stage is isolated to prevent cascading failures.

---

## Design Decisions

### Hybrid Processing Model

Face recognition runs locally rather than in the cloud to:

- Reduce infrastructure costs
- Improve performance control
- Maintain privacy boundaries

### Queue-Based Workflow

Instead of synchronous processing:

- Jobs are queued
- Workers pull tasks when available
- This avoids blocking user interactions

### Modular Separation

Frontend, backend, storage, worker, and messaging are separated to allow:

- Independent scaling
- Easier debugging
- Safer updates

---

## Scalability Considerations

The architecture supports future scaling strategies such as:

- Multiple workers processing jobs in parallel
- Distributed storage backends
- Additional delivery channels beyond Telegram
- GPU-enabled worker upgrades

The modular structure ensures that adding new features does not require redesigning the entire system.

---

## Related Documentation

For more detailed flows, see:

- `/docs/ui-flow/`
- `/docs/database/`
- `/docs/worker-pipeline/`

